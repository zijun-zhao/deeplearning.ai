

### This file will record all problems I have encountered during studying the course W4111 in 2020 Spring.
* Prof. Donald F. Ferguson's page for this course [GitHub home page](https://donald-f-ferguson.github.io/IntroToDatabases/)
* Prof. Donald F. Ferguson's [Repository/Project](https://github.com/donald-f-ferguson/IntroToDatabases)

### Table of Contents

1. [Lecture1&2-24Jan](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture1&2_Intro&Overview.md)
2. [Lecture3-31Jan](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture3.md)
3. [Lecture4-7Feb](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture4_ERModel_SQL.md)
4. [Lecture5-14Feb](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture5_ERModel_SQL.md)
5. [Lecture6-21Feb](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture6_RelationalAlgebra.md)
6. [Lecture7-28Feb](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture7_Wrap_up.md)
7. [Lecture8-6Mar](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture8_EndModule_I.md)
8. [Lecture9-13Mar](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture9_Disks&IO&Index.md)
9. [Lecture10-29Mar&3Apr](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture10_Index&QueryProcessing.md)
10. [Lecture11-10Apr](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture11_QueryProcessing&Transactions&Recovery.md)
11. [Lecture12-17Apr](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/Lecture12_Transactions.md)

* Course Modules 
	* Transaction Concept
	* Transaction State
	* Concurrent Executions
	* Serializability
	* Recoverability
	* Implementation of Isolation
	* * Transaction Definition in SQL
	* Testing for Serializability.

  
## 17 Apr 2020


1. Transaction Concept
  * A transaction is a **unit of program execution** that accesses and  possibly updates various data items.
  * E.g., transaction to transfer $50 from account A to account B:
        1.	read(A)
        2.	A := A ‚Äì 50
        3.	write(A)
        4.	read(B)
        5.	B := B + 50
        6.	write(B)
 * Two main issues to deal with:
   * Failures of various kinds, such as hardware failures and system crashes
   * Concurrent execution of multiple transactions





* The edges are the flow of data. The data flowing is a relation.
    * Base table.
    * Derived table.
6.Logical Plan
> Here is an example from [this website]()

Logical Query|PlanDistributed Query Plan
---|---
![fig1](https://gerardnico.com/_media/data/type/relation/engine/logical_query_plan.png?w=300&tok=0fc4d4) | ![fig1](https://gerardnico.com/_media/data/type/relation/engine/distributed_query_plan.png?ezimgfmt=rs:607x454/rscb1/ng:webp/ngcb1)
```sql
SELECT
    item.brand,
    SUM(price)
FROM
    sales,
    item
WHERE
    sales.item_key = item.item_key
GROUP BY item.brand
```
* The plan executes in stages (bottom up).
* Some stages can have parallelism, e.g. multiple threads to improve CPU utilization while processing I/O.
    * Here 'Scan on item' is a select, two different threads can run at the same time: 'scan on item' and 'scan on sales'.
    
7. Query Optimization from [Database Systems](http://infolab.stanford.edu/~ullman/dscb.html)
    * Which of the algebraically equivalent forms of a query leads to the **most efficient** algorithm for answering the query? 
    * For each operation of the selected form, what algorithm should we use to implement that operation?
    * How should the operations pass data from one to the other, e.g., in a pipelined fashion, in main-memory buffers, or via the disk?
* Each of these choices depends on the metadata about the database. Typical metadata that is available to the query optimizer includes: the size of each relation; statistics such as the approximate number and frequency of different values for an attribute; the existence of certain indexes; and the layout of data on disk.
    
8. Three Core Optimization Techniques

* Query rewrite: Transform the logical query into an equivalent, more efficient query (lower cost query), e.g.
   * R ‚ãà S is the same as S ‚ãà R
   * ùûº(R ‚ãà S) is the same at ùûº(R) ‚ãà ùûº(S)
       * cost of **join** is size of table R times size of table S. 100*100=10 000. R ‚ãà S is row(R)*row(S), the select cost is 1.
       * cost of ùûº(R) is 1 if it is performed on the primary key, therefore cost of ùûº(R) ‚ãà ùûº(S) is 100
   * ùõÖ(R ‚ãà S) = ùõÖ(R) ‚ãà ùõÖ(S), where ùõÖ is the distinct operator.
* Access path selection: Which index to choose?
* Operator implementation selection:
    * There are several related implementations for each operator
    * For example,
        * Sort Scan
        * Sort Join
        * Hash Join
        * Hash Distinct
9. Statistics and Catalogs
    * Need information about the relations and indexes involved.  Catalogs typically contain at least:
        * **tuples (NTuples)** and **pages (NPages)** for each relation.
        * **distinct key values (NKeys)** and **NPages** for each index.
        * **Index height, low/high key values (Low/High)** for each tree index.
    * Catalogs updated periodically.
    * Updating whenever data changes is too expensive; lots of approximation anyway, so slight inconsistency ok.
    * More detailed information (e.g., histograms of the values in some field) are sometimes stored
10. The Catalog Contains
    * For each table
        * Table name, storage information.
        * Attribute name and type for each column.
        * Index name, type and indexed attributes
        * Constraints
    * Statistical information
        * Cardinality of each table
        * Size: No. of blocks.
             * If there are lots of rows and rows are big, there will be a lot of blocks
        * Index cardinality: Number of **distinct key values** for each index
            * If the key is primary/unique, then the number of dinstinct key value = number of rows; But if it is a secondary index, the number of index will be less than number of rows.
        * Index size: Number of blocks for each index.
             * An index is a subset of the column.
        * Index tree height
            * how many levels
        * Index ranges
11. Access Path
* Every relational operator accepts one or more tables as input.The operators ‚Äúaccesses the tuples‚Äù in the tables. 
* There are two ‚Äúways‚Äù to retrieve the tuples
* Scan the relation (via blocks)
* Use an index and matching condition.
    * if there is a condition in the where clause. No need to scan the table, but need to go through the index. Therefore the height of index tree matters.
* A selection condition is in **conjuctive normal** form if
    * Each term is column_name op value
         * compare column against the value
    * op is one of <, <=, =, >, >=, <>
    * The terms are combined, e.g. (nameLast = ‚ÄòFerguson‚Äô) AND (ab > 500)
        * and
    * CNF allows using a matching index for the access path.
* The engine can select a *matching index*
    * A Hash Index on (c1,c2,c3) if the condition is *c1=x AND c2=y AND c3=z*.
    * A Tree Index if the condition is (can be ordered as)
        * c1 op value
            * 'nameLast = ‚ÄòFerguson‚Äô', in the set of people last name is Fergusion, then return whose ab>500. Even every rows are in a different blocks, fewer blocks are needed to read
        * c1 op x AND/OR c2 op y
        * ‚Ä¶ ‚Ä¶
    * Many indexes may match, and the engine chooses the most **selective match**(number of tuples or blocks) that match.
        * the most selective index is the one with the fewest entries. 
    
Remember keys are smaller than the rows. So we can get many key values. Making the index tree two levels deep,  1:09Ôºö00
12. Query evalueation

    * Alternative ways of evaluating a given query
        * Equivalent expressions
        * Different algorithms for each operation
    * An example of pushing down the select ![Image of Yaktocat](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/imgs/10Apr_4.png)
    
13. SELECTion Pushing
* Consider two relations
    * StarsIn(title, year, star_name)
    * Movies(title, year, length, genre, studio_name, producer_no)
* Find everyone who starred in a comedy movie in 1996
    * Query 1: 
```sql
SELECT star_name,  FROM StarsIn JOIN Movies ON
StarsIn.title=Movies.title AND StarsIn.year=Movies.year
WHERE StarsIn.year=1996 and Movies.genre='Comedy'
```
* Assume both tables have 1000 rows. Then the JOIN operation cost 1 000 000
    * Query 2:
```sql
SELECT star_name FROM (SELECT star_name, title, year FROM StarsIn WHERE year=1996 as a) 
JOIN
(SELECT title, year, genre FROM Movies WHERE year=1996 AND genre=‚ÄòComedy‚Äô)	
ON a.year=b.year and a.title=b.title
```
* Assume both tables have 1000 rows. Two select may be 30 and 20. Then the final result is 600. Table becomes smaller.
* SELECT on a JOIN of table R and table S compares O(#(R)*#(S)) tuples
    * Have to load relevant blocks
    * Compare the tuples to compute the JOIN
    * Scan the JOIN to apply the SELECT
* Pushing the SELECT(s) through the JOINs
    * May vastly **reduce the size of the table** to JOIN
    * Only JOIN tuples that could be selected in the WHERE clause.
    
You can also push project through JOIN, but this **reduces the size of the data**, not the number of tuples examined.
14. Join: 
> Default choice is Index Nested Loops
```
for each tuple r in R do
	for each tuple s in S where ri == sj  do
		add <r, s> to result
  
```
* For every row in R, and for every record in S, if equal then put in the record. If there is N rows in S, the second operation will run N times for each iteration. 
    * But in fact, when operating 'for each tuple s in S where ri == sj', we know the value for r, we can use that as an index. 
        * The reason to swap the table: we want the index to be in the inside to eliminate the need to scan.
        * If both them have clustering index, we want the clustered index to be inside. If the index is not cluster, the order of index does not guarantee the order of the files. If the index is clustered, when you hit the first record in the block, we can eliminate the block I/O.
    * If there is an index on the join column of one relation (say S), can make it the inner and exploit the index.
        * Cost:  M + ( (M*pR) * cost of finding matching S tuples) 
        * M=#pages of R, pR=# R tuples per page
    * For each R tuple, cost of probing S index is about 1.2 for hash index, 2-4 for B+ tree.  Cost of then finding S tuples (assuming Alt. (2) or (3) for data entries) depends on clustering.
        * Clustered index:  1 I/O (typical), unclustered: upto 1 I/O per matching S tuple.


15. Join: Sort-Merge ((R)‚®ùi=j(S))
![Image of Yaktocat](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/imgs/10Apr_5.jpg)
* Sort R and S on the join column, then scan them to do a ``merge‚Äô‚Äô (on join col.), and output result tuples.
    * Advance scan of R until current R-tuple >= current S tuple, then advance scan of S until current S-tuple >= current R tuple; do this until current R tuple = current S tuple.
    * At this point, all R tuples with same value in Ri (current R group) and all S tuples with same value in Sj (current S group) match;  output <r, s> for all pairs of such tuples.
    * Then resume scanning R and S.
* R is scanned once; each S group is scanned once per matching R tuple.  (Multiple scans of an S group are likely to find needed pages in buffer.) 1:19:18 fig
    * Sort R based on the join staff. Then when doing join, we only need to go through the other table once. The sort merge is cheaper than the nexted loop.
16. Recall Selection Operation
* File scan 
    * Algorithm A1(**linear search**).  Scan each file block and test all records to see whether they satisfy the selection condition. 
        * Cost estimate = brblock transfers + 1 seek
            * b<sub>r</sub> denotes number of blocks containing records from relation r
    * If selection is on a key attribute, can stop on finding record
        * cost = (b<sub>r</sub>/2) block transfers + 1 seek
    * Linear search can be applied regardless of 
        * selection condition or
        * ordering of records in the file, or 
        * availability of indices 
    * Note: binary search generally does not make sense since data is not stored consecutively
        * except when there is an index available, 
        * and binary search requires more seeks than index search
* Index scan: search algorithms that use an index
    * selection condition must be on search-key of index.
    * A2 (clustering index, equality on key).  Retrieve a single record that satisfies the corresponding equality condition  
        * Cost= (h<sub>i</sub>+ 1) * (t<sub>T</sub>+ t<sub>S</sub>)
    * A3 (clustering index, equality on nonkey)Retrieve multiple records.
        * Records will be on consecutive blocks 
            * Let b = number of blocks containing matching records 
            * Cost= h<sub>i</sub>* (t<sub>T</sub>+ t<sub>S</sub>)+ t<sub>S</sub>+ t<sub>T</sub>* b
    * A4(secondary index, equality on key/non-key). 
        * Retrieve a single record if the search-key is a candidate key 
            * Cost = (hi+ 1) * (tT+ tS) 
        * Retrieve multiple records if search-key is not a candidate key 
        * each of n matching records may be on a different block 
            * Cost =  (hi+ n) * (tT+ tS)
            * Can be very expensive!
       > Each index entry is going to refer to a block, but we still need to do block I/O in the worst case
16. Several different algorithms to implement joins
* Nested-loop join
* Block nested-loop join
* Indexed nested-loop join
* Merge-join
* Hash-join
17. Nested-loop join
	* To compute the theta join        r ‚®ùŒ∏s
```sql
for each tuple tr in r do begin
	for each tuple ts  in s do begin
		test pair (tr,ts) to see if they satisfy the join condition Œ∏
			if they do, add tr¬∑ts to the result.
		end
	end
```
* The default join
	* r  is called the **outer relation** and s the **inner relation** of the join.
	* Requires no indices and can be used with any kind of join condition.
	* Expensive since it examines every pair of tuples in the two relations. 
17. Block nested-loop join basicly operates on the block level, the idea is that we go through the block therefore we will not read block over and over again. Each block will only be read once,
 	* Variant of nested-loop join in which every block of inner relation is paired with every block of outer relation.
```sql
for each block Br of r do begin
	for each block Bs of s do begin
		for each tuple tr in Br  do begin
			for each tuple ts in Bs do begin
				Check if (tr,ts) satisfy the join condition
					if they do, add tr 
					ts to the result.
			end
		end
	end
end
```

18. Indexed nested-loop join
	* Index lookups can replace file scans if
		* join is an equi-join or natural join and
		*an index is available on the inner relation‚Äôs join attribute
			* Can construct an index just to compute a join.
> Sometimes even there are not index on the table, the database will build an index. If the index is ordered, then it is similar to a sort. However, if the only comparison operator is the equality, we can build a hash table.

19. Hash-Join![Image of Yaktocat](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/imgs/10Apr_6.jpg)
	* Applicable for equi-joins and natural joins.
	* A hash function h is used to partition tuples of both relations 
	* When we hash a table, for a given hash value, all the rows and columns hashed to the value are in the same bucket. By only doing **equality**, we can sort a table
		* If the bucket is too large, we can load it in memory.
20. Complex Joins
	* Join with a conjunctive condition: r‚®ùŒ∏<sub>1</sub>‚àßŒ∏<sub>2</sub>‚àß...Œ∏<sub>n</sub>S
		* Either use nested loops/block nested loops, or
		* Compute the result of one of the simpler joins r ‚®ùŒ∏<sub>i</sub>s
			* final result comprises those tuples in the intermediate result that satisfy the remaining conditions
				* Œ∏<sub>1</sub>‚àßŒ∏<sub>2</sub>‚àß...Œ∏<sub>n</sub>

21. Materialization
	* Materialized evaluation: evaluate one operation at a time, starting at the lowest-level.  Use intermediate results materialized into temporary relations to evaluate next-level operations.
	* Example of compute and store œÉbuilding="Warson"(department) ![Image of Yaktocat](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/imgs/10Apr_7.jpg)
	* Two ways to run this query

Method_1|Method_2
---|---
Run œÉbuilding="Warson"(department) completely and obtain a table| Run the select, every time a tuple comes out then do the join

* Using **pipeline**, we can run the join the same time as running the select
	* Pipelined evaluation:  evaluate several operations simultaneously, passing the results of one operation on to the next.
	E.g., in previous expression tree, don‚Äôt store result of œÉbuilding="Warson"(department)
		* instead, pass tuples directly to the join..  Similarly, don‚Äôt store result of join, pass tuples directly to projection. 
	* Much cheaper than materialization: no need to store a temporary relation to disk.
	* Pipelining may not always be possible ‚Äì e.g., sort, hash-join. 
	* For pipelining to be effective, use evaluation algorithms that generate output tuples even as tuples are received for inputs to the operation. 
	* Pipelines can be executed in two ways:  demand driven and producer driven 
	
	
22. Core Transaction Concept is ACID Properties
	* Atomic: Transaction cannot be subdivided.
		* if there are several transaction, either all of them happen, or none of them happen
	* Consistent
		* transforms database from one consistent state to another consistent state
	* Isolated: Transactions execute independently of one another
		* Database changes not revealed to users until after transaction has completed
	* Durable: Database changes are permanent
		* The permanence of the database‚Äôs consistent state

23. Atomicity
	* A transaction is a logical unit of work that must be either **entirely** completed or **entirely** undone. (All writes happen or none of them happen).
		* In the following example, the two update must both be completed, or not be completed.
```sql
def transfer(source_acct_id, target_acct_id, amount)
	Check that both accounts exist.
	IF is_checking_account(source_acct_id)
		Check that (source_acct.balance-amount) > source_account.overdraft_limit
	ELSE
		Check that (source_count.balance-amount) >source_account.minimum_balance
	Update source account
	Update target account.
	INSERT a record into transfer tracking table.
```

24. Simplistic Approach

* sql has an auto commit function, by using the following verbs we can decide whether to commit or not  ![Image of Yaktocat](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/imgs/10Apr_8.jpg)
	* start transction
	* commit
	* roll back
* Define a variable that exists while connected. Once disconnected the variable will go away
```sql
select balance into @a_balance from banking_account where id = 1;
select @a_balance;

select balance into @b_balance from banking_account where id = 2;
select @b_balance;
```
* Suppose a 50 dollars transcaction
```sql
update banking_account set balance = @a_balance-50 where id=1;
update banking_account set balance = @a_balance+50 where id=2;
```
* This will fail at the second update. We can see the change of id = 1 on the engine we run the query, but we are not able to see the change on another engine
* Then, using **roll back**, run the following query
```sql
start transction;
select balance into @a_balance from banking_account where id = 1;
select @a_balance;

select balance into @b_balance from banking_account where id = 2;
select @b_balance;

update banking_account set balance = @a_balance-50 where id=1;
update banking_account set balance = @a_balance+50 where id=2;

commit;
```
* This modification can be seen by others.

25. There are several problems with the simplistic approach.
	* The approach does not solve the problem
		* Some write might succeed.
		* Some might be interrupted by the failure, or require retry.
	* Writes may be random and scattered. N updates might
			* Change a few bytes in N data frames
			* A few bytes in M index frames
		* Transaction rate becomes bottlenecked by write I/O rate, even though a relative small number of bytes change/transaction.
	* Written frames must be held in memory.
		* Lots of transactions
		* Randomly writing small pieces of lots of frames.
		* Consumes lots of memory with pinned pages.
	* Degrades the performance and optimization of the buffer. 
		* The optimal buffer replacement policy wants to hold frames that will be reused.
		* Not frames that have been touched and never reused.
		
* Implementation Subsystems 
	* Query processor schedules and executes queries.
		* Return to user once the transaction is done
	* Buffer manager controls reading/writing blocks/frames to/from disk.
	* Log manager journals/records events to disk (**log**), e.g.
		* Transaction start/commit/abort
		* Transaction update of a block and previous value of data.
			* New value of the bytes will be used. An update may touch one 64 key block, then log record will keep the block number, the offset, the old data and the new data(like 20 bytes).
	* Transaction manager coordinates query scheduling, buffer read/write and logging to ensure ACID.
	* Recovery manager processes log to ensure ACID even after transaction or system **failures**.
	* Log is a special type of block file used by the system to optimize performance and ensure ACID.

* Writing is slow, so why does writing log file help? ![Image of Yaktocat](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/imgs/10Apr_9.jpg)
> When we do something like an update, the data goes into the memory, but the update also goes in to the log stream. While each block is 64k, the only thing we need in the log file is the block id, offset, old&new value, less than the block size. Therefore we can run lots of transaction at the same time which all write to the log stream. When a log pages full, or sb does a commit, we can force the log out without forcing the frame. If crash happens, we can process the log and we know which page is dirty but has not been written to the disk.

26. Write Ahead Logging
* DBMS (Redo processing)  ![Image of Yaktocat](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/imgs/10Apr_10.jpg)
	* Write log events from all transactions into a single log stream.
	* Multiple events per page
	* Forces (writes) log record on COMMIT/ABORT
		* Single block I/O records many updates
		* Versus multiple block I/Os, each recording a single change.
		* All of a transaction‚Äôs updates recorded in one I/O versus many.
	* If there is a failure
		* DBMS sequentially reads log.
		* Applies changes to modified pages that *were not saved to disk*.
		* Then resumes normal processing.
		> How a database system efficient implement atomicity: Before the transaction is committed, write the current log page to the disk, once that page is written to the disk, that transaction is committed and people can recover the data if we fail before writing the pages back. The state of the our data is the state of block on the disk+any updates that were lost in memory. We figure the latter out from the log
	* If we force a dirty page on the disk. Committed changes in memory and uncommitted changes in disk. We need to undo the update, during recovery, the database engine process the log and redo all the updates that committed but not written to disk. Then undo all the updates that were written to disk without commit.
		* But why people will write an uncommit change to disk?


* Force every write to disk?
	* Poor response time.
	* But provides durability.
	> like change 3 bytes but write 64k. 
* Best I/O is guaranteed by Steal and No Force. This is efficient since buffer space is adequate. To implement No Force, we need do the **redo processing**. To implement Steal policy, we need to do the **undo processing** during recovery.
	-| No Steal | Steal
	---|---|---
	Force|Trivial|
	No Force|  |Desired
	
	
* Steal buffer-pool frames from uncommitted transactions?
	* If not, poor performance/caching performance
	* If yes, how can we ensure atomicity?
		* Uncommitted updates on disk
	> If we need the block, can we write the uncommitted page to the disk. Steal means **if I need memory, we will write an uncommitted block to disk.**
	
* DBMS (Undo processing): *forcing uncommitted transaction to disk, but do not forcing updates to disk.*
	* Enable steal policy to improve cache performance by
		* Avoiding lots of pinned pages
		* Unlikely to be reused soon.
	* Before stealing
		* Force log record to disk.
		* Update log entry has data record
			* Before image
			* After image
	* If there is a failure
		* DBMS sequentially reads log.
		* Undoes changes to
			* modified pages, uncommitted pages
			* That were saved to disk.
		* Then resumes normal processing.

27. ARIES Algorithm = Algorithms for Recovery and Isolation Exploiting Semantics

28. Durability: Write changes to disk trivially achieves durability.
	* DBMS engine uses **write-ahead-logging to**
		* Achieve durability
		* But with better performance through more efficient caching and I/O.
	* Well, disks fail. How is that durable.
		* RAID and other solutions.
		* Disk subsystems, including entire RAID device, fail ÔÉ†
			* Duplex writes
			* To independent disk subsystems.
	* Well, there are earthquakes, floods, etc. 
29. Locking and Concurrency
	* Application code requires a ‚Äúprocessor/CPU‚Äù to **execute**.
	* The operating system (or execution server) allocates the CPU to a program/subprogram that is ‚Äùready to run,‚Äù e.g.
		* Not waiting for an I/O to complete.
		* Not waiting for user input.
		* etc.
	* The operating system suspends a program and reassigns a CPU when the application performs a blocking operation, e.g.
		* Make a remote function call.
		* A disk I/O is necessary.
		* The program must wait for a physical or logical resource, e.g. ‚Äúa lock.‚Äù
	* Acquiring a database lock is (potentially) a blocking operation that suspends the application until the lock request is satisfied. 
	> Many processes are running at the same time
20. Locking  ![Image of Yaktocat](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/imgs/10Apr_11.jpg)
	* Write log events from all transactions into a single log stream.
* Transaction Manager
	* Intercepts all database operations.
	* Acquires/checks locks on
		* Records
		* Pages
		* Index pages
	* Suspends and queues an operation
		* In another active, executing op.
		* Has a conflicting lock.
	* Restarts queued operations when conflicting locks are released.
21. Schedule: order of operation and transaction
	* Serial Schedule: its actions consist of all the actions of one transaction, then all the actions of another transaction, and so on. **No mixing of the actions is allowed**
		* As long as we run one transaction from the beginning to end, the result is consistnet
	* Assume there are three
		* concurrently executing transactions
		* T1, T2 and T3
	* The transaction manager
		* Enables concurrent execution
		* But schedules individual operations
		* To ensure that the final DB state
		* Is equivalent to one of the following schedules
			* T1, T2, T3
			* T1, T3, T2
			* T2, T1, T3
			* T2, T3, T1
			* T3, T1, T2
			* T3, T2, T1
> The transcation manager has the ability to stop the transaction on any operation. The transcation manager can merge the transactions in one stream. When all the transactions are committed, the database will be in some states. For the three transactions, possible schedules have 6 possible schedules-> Each transaction is run from the start to end. As long as the actual order matches one of them, the database is still in a consistent state. The Concurrent execution is called **serializable**: an equivalent serial schedule mateches what the engine does.
* Locking is to guarantee when transaction is commit, the result is equivalent to a serial schedule.

22. Aborting a Transaction
	* If a transaction Ti is aborted, all its actions have to be undone.  Not only that, if Tj reads an object last written by Ti,  Tj must be aborted as well!
	* Most systems avoid such **cascading aborts** by releasing a transaction‚Äôs locks only at commit time.
		* If Ti writes an object, Tj can read this only after Ti commits.
	* In order to undo the actions of an aborted transaction, the DBMS maintains a log in which every write is recorded.  This mechanism is also used to recover from system crashes:  all active Xacts at the time of the crash are aborted when the system comes back up.
	> Phase locking won't allow cascading aborts.
23. In database environment, we do not set locking explicityly, instead, we set the **isolation level** at the beginning of the transaction.

![Image of Yaktocat](https://github.com/zijun-zhao/fishLearning/blob/master/COMS4111/imgs/10Apr_12.jpg)
